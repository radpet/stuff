{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to machine learning.\n",
    "## Natural language processing.\n",
    "___\n",
    "   \n",
    "*Radoslav Petkov*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### About me\n",
    "___\n",
    "\n",
    "* Contact: https://www.linkedin.com/in/radoslav-petkov-8a4a53144/\n",
    "* Sofia University, Computer Science, Bsc (2nd year)\n",
    "* Has been working for Sirma since the end of 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "___\n",
    "\n",
    "* Supervised vs Unsupervised machine learning\n",
    "* Representation of words and sentences\n",
    "* Autoencoders\n",
    "* Sequence to sequence models\n",
    "* Memory Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised vs Unsupervised\n",
    "\n",
    "![](un_supervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Supervised\n",
    "___\n",
    "\n",
    "###  $ y= a*X + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "___\n",
    "\n",
    "### $y \\in /R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "boston = load_boston()\n",
    "\n",
    "dataset = pd.DataFrame(data=boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "dataset[\"Target\"] = boston[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset[dataset[\"Target\"] < 10][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset[(dataset[\"Target\"] > 10) & (dataset[\"Target\"] < 30)][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset[dataset[\"Target\"] > 45][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "___\n",
    "\n",
    "### $y \\in [0..c]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "pd_iris = pd.DataFrame(data=iris[\"data\"], columns=iris[\"feature_names\"])\n",
    "pd_iris[\"Type\"] = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 2][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised\n",
    "#### $ y$ is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Code source: GaÃ«l Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "# with slight modifications by me\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "plt.close(\"all\")\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_5', KMeans(n_clusters=5)),\n",
    "                       ('k_means_iris_3', KMeans(n_clusters=3))]\n",
    "\n",
    "titles = ['8 clusters', '5 clusters', '3 clusters']\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(12,12))\n",
    "\n",
    "for i, (name, est) in enumerate(estimators):\n",
    "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.dist = 12\n",
    "\n",
    "# Plot the ground truth\n",
    "ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.axes\n",
    "for ax in fig.axes:\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "# fig.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is overfitting, underfitting and what is good fit?\n",
    "\n",
    "* Overfitting - high score on train and low on test\n",
    "* Underfitting - low score on both train and test\n",
    "* Generalization - high score on both train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural-language processing (NLP)\n",
    "### How the computer works with text?\n",
    "\n",
    "![](natural-language-processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### What is one hot encoding of words?\n",
    "\n",
    "Simply said, it is representation where each word is vector with size equal to the vocabulary size and there is 1 at the index equal to the word index.\n",
    "\n",
    "Imagine you have the following corpus of words: *Machine*, *learning*, *rocks*.\n",
    "\n",
    "***One representation would be :***\n",
    "\n",
    "Machine -> [1 0 0],  learning -> [0 1 0], rocks -> [0 0 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ### What about Bag of Words?\n",
    "\n",
    "Each sentence is vector with size equal to the vocablary size storing the occurances of each word in the sentence.\n",
    "There are several additional modifications such as:\n",
    "* **TF-IDF (Term frequency - Inverse document frequency) **\n",
    "\n",
    "    We assign weights of each word instead of just occurances.The weights tend to filter out common terms so the lower the weight is the more occurances the word has.\n",
    "    TF is the ration between the count of the word in the current sentence and all words in the sentence.\n",
    "    IDF is the logarithmically scaled fraction of all documents in the corpus with the documents that contain the given word.\n",
    "    \n",
    "    TF-IDF has various weighting schemes, for example :\n",
    "    $ \\mbox{tf-idf}_{t,d} = (1 +\\log \\mbox{tf}_{t,d}) \\cdot \\log \\frac{N}{\\mbox{df}_t} $\n",
    "* **Hashing**\n",
    "\n",
    "    Instead of using dictionary to vectorize, a special hash function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "___\n",
    "\n",
    "Lets have the following corpus: *machine*, *learning*, *rocks*, *this*, *robot*\n",
    "\n",
    "And the following sentences:\n",
    "\n",
    "* *machine learning rocks*\n",
    "* *this machine learning rocks*\n",
    "* *this robot rocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"this computer\", \"this machine\", \"this robot\", \"this text\"]\n",
    "corpus_transformed = CountVectorizer().fit(corpus)\n",
    "corpus_tf_idf  = TfidfVectorizer().fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(corpus_transformed.vocabulary_)\n",
    "for (sentence, transf) in zip(corpus, corpus_transformed.transform(corpus).toarray()):\n",
    "    print(sentence, transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_tf_idf.vocabulary_)\n",
    "for (sentence, transf) in zip(corpus, corpus_tf_idf.transform(corpus).toarray()):\n",
    "    print(sentence, transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### Hmm, now the fancy word vectors\n",
    "\n",
    "Vectors from lattent space formed by the corpus used for training unsupervised model.\n",
    "They can catch the 'semantic meaning of a word'.\n",
    "\n",
    "![](word_embedding2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](word_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/media/radoslav/6906F83679A14133/Download/glove/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['walking', 'swam'], negative=['swimming'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['France', 'Sofia'], negative=['Paris'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['queen', 'man'], negative=['woman'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now lets try what such a model can do on one of those IQ quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.doesnt_match([\"apple\", \"banana\", \"orange\", \"bread\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "___\n",
    "\n",
    "![](autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence to sequence models\n",
    "___\n",
    "![](encdec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](seq2seq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory Networks\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can they do?\n",
    "\n",
    "* Joe went to the *kitchen*. Fred went to the *kitchen*. Joe picked up the *milk*.\n",
    "* Joe travelled to the *office*. Joe left the *milk*. Joe went to the *bathroom*.\n",
    "* **Where is the milk now?** A: *office*\n",
    "* **Where is Joe?** A: *bathroom*\n",
    "* **Where was Joe before the office?** A: *kitchen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Wtf how?\n",
    "![](study-of-end-to-end-memory-networks-9-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](thank-you-for-your-attention-now-its-time-for-questions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sources of some of the images:\n",
    "\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd\n",
    "* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "* https://www.celebros.com/blog/five-benefits-to-choosing-natural-language-processing-for-on-site-search.html\n",
    "* https://www.tensorflow.org/tutorials/seq2seq\n",
    "* https://image.slidesharecdn.com/presentationrev1-170221023043/95/study-of-end-to-end-memory-networks-9-638.jpg?cb=1487644455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful references:\n",
    "\n",
    "* https://github.com/fmi/machine-learning-lectures\n",
    "* https://www.kaggle.com/\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://en.wikipedia.org/wiki/Feature_hashing\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "* https://www.tensorflow.org/tutorials/seq2seq\n",
    "* https://github.com/facebook/MemNN\n",
    "* https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>All links above are interesting and useful if you want to deep dive in the world of machine learning.</center>\n",
    "\n",
    " ### <center>The end, thanks!</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
