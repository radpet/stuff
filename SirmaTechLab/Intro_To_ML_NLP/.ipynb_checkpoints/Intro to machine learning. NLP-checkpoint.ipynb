{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to machine learning.\n",
    "## Natural language processing.\n",
    "___\n",
    "   \n",
    "*Radoslav Petkov*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### About me\n",
    "___\n",
    "\n",
    "* Contact: https://www.linkedin.com/in/radoslav-petkov-8a4a53144/\n",
    "* Sofia University, Computer Science, Bsc (2nd year)\n",
    "* Has been working for Sirma since the end of 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "___\n",
    "\n",
    "* Supervised vs Unsupervised machine learning\n",
    "* Representation of words and sentences\n",
    "* Autoencoders\n",
    "* Sequence to sequence models\n",
    "* Memory Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised vs Unsupervised\n",
    "\n",
    "![](un_supervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Supervised\n",
    "___\n",
    "\n",
    "### Example of simple linear model: $ y= X*W + b$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression\n",
    "___\n",
    "\n",
    "### $y \\in /R $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "boston = load_boston()\n",
    "\n",
    "dataset = pd.DataFrame(data=boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "dataset[\"Target\"] = boston[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>20.0849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.368</td>\n",
       "      <td>91.2</td>\n",
       "      <td>1.4395</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>285.83</td>\n",
       "      <td>30.63</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>16.8118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.277</td>\n",
       "      <td>98.1</td>\n",
       "      <td>1.4261</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>30.81</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS  NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "384  20.0849  0.0   18.1   0.0  0.7  4.368  91.2  1.4395  24.0  666.0   \n",
       "385  16.8118  0.0   18.1   0.0  0.7  5.277  98.1  1.4261  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  Target  \n",
       "384     20.2  285.83  30.63     8.8  \n",
       "385     20.2  396.90  30.81     7.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"Target\"] < 10][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "\n",
       "   PTRATIO      B  LSTAT  Target  \n",
       "0     15.3  396.9   4.98    24.0  \n",
       "1     17.8  396.9   9.14    21.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[(dataset[\"Target\"] > 10) & (dataset[\"Target\"] < 30)][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.46336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>7.489</td>\n",
       "      <td>90.8</td>\n",
       "      <td>1.9709</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>374.43</td>\n",
       "      <td>1.73</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.83377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>7.802</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>389.61</td>\n",
       "      <td>1.92</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "161  1.46336  0.0  19.58   0.0  0.605  7.489  90.8  1.9709  5.0  403.0   \n",
       "162  1.83377  0.0  19.58   1.0  0.605  7.802  98.2  2.0407  5.0  403.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  Target  \n",
       "161     14.7  374.43   1.73    50.0  \n",
       "162     14.7  389.61   1.92    50.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset[\"Target\"] > 45][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "___\n",
    "\n",
    "### $y \\in [0..c]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "pd_iris = pd.DataFrame(data=iris[\"data\"], columns=iris[\"feature_names\"])\n",
    "pd_iris[\"Type\"] = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd_iris [ pd_iris[\"Type\"] == 2][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss, the function for optimization\n",
    "----\n",
    "Loss functions measure how bad (based on the labeled data) our model predicts and we want to find good enough weights that minimize it.\n",
    "\n",
    "* Mean Squared Error (MSE): $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* LogLoss: $- \\frac{1}{n} \\sum\\limits_{i=1}^n (y_i \\cdot ln(\\hat{y_i}) + (1-y_i) \\cdot ln(1-\\hat{y_i}) )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised\n",
    "#### $ y$ is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Code source: GaÃ«l Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "# with slight modifications by me\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "plt.close(\"all\")\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_5', KMeans(n_clusters=5)),\n",
    "                       ('k_means_iris_3', KMeans(n_clusters=3))]\n",
    "\n",
    "titles = ['8 clusters', '5 clusters', '3 clusters']\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(12,12))\n",
    "\n",
    "for i, (name, est) in enumerate(estimators):\n",
    "    ax = fig.add_subplot(2, 2, i+1, projection='3d')\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.dist = 12\n",
    "\n",
    "# Plot the ground truth\n",
    "ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.axes\n",
    "for ax in fig.axes:\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "# fig.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is overfitting, underfitting and what is good fit?\n",
    "\n",
    "* Overfitting - high score on train and low on test\n",
    "* Underfitting - low score on both train and test\n",
    "* Generalization - high score on both train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural-language processing (NLP)\n",
    "### How the computer works with text?\n",
    "\n",
    "![](natural-language-processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### What is one hot encoding of words?\n",
    "\n",
    "Simply said, it is representation where each word is vector with size equal to the vocabulary size and there is 1 at the index equal to the word index.\n",
    "\n",
    "Imagine you have the following corpus of words: *Machine*, *learning*, *rocks*.\n",
    "\n",
    "***One representation would be :***\n",
    "\n",
    "Machine -> [1 0 0],  learning -> [0 1 0], rocks -> [0 0 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ### What about Bag of Words?\n",
    "\n",
    "Each sentence is vector with size equal to the vocablary size storing the occurances of each word in the sentence.\n",
    "There are several additional modifications such as:\n",
    "* **TF-IDF (Term frequency - Inverse document frequency) **\n",
    "\n",
    "    We assign weights of each word instead of just occurances.The weights tend to filter out common terms so the lower the weight is the more occurances the word has.\n",
    "    TF is the ration between the count of the word in the current sentence and all words in the sentence.\n",
    "    IDF is the logarithmically scaled fraction of all documents in the corpus with the documents that contain the given word.\n",
    "    \n",
    "    TF-IDF has various weighting schemes, for example :\n",
    "    $ \\mbox{tf-idf}_{t,d} = (1 +\\log \\mbox{tf}_{t,d}) \\cdot \\log \\frac{N}{\\mbox{df}_t} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Hashing**\n",
    "\n",
    "    Instead of using dictionary to vectorize, a special hash function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "___\n",
    "\n",
    "Lets have the following corpus: *machine*, *learning*, *rocks*, *this*, *robot*\n",
    "\n",
    "And the following sentences:\n",
    "\n",
    "* *machine learning rocks*\n",
    "* *this machine learning rocks*\n",
    "* *this robot rocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"this computer\", \"this machine\", \"this robot\", \"this text\"]\n",
    "corpus_transformed = CountVectorizer().fit(corpus)\n",
    "corpus_tf_idf  = TfidfVectorizer().fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(corpus_transformed.vocabulary_)\n",
    "for (sentence, transf) in zip(corpus, corpus_transformed.transform(corpus).toarray()):\n",
    "    print(sentence, transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(corpus_tf_idf.vocabulary_)\n",
    "for (sentence, transf) in zip(corpus, corpus_tf_idf.transform(corpus).toarray()):\n",
    "    print(sentence, transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### Hmm, now the fancy word vectors\n",
    "\n",
    "Vectors from lattent space formed by the corpus used for training unsupervised model.\n",
    "They can catch the 'semantic meaning of a word'.\n",
    "\n",
    "![](word_embedding2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](word_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/media/radoslav/6906F83679A14133/Download/glove/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['walking', 'swam'], negative=['swimming'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['France', 'Sofia'], negative=['Paris'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.most_similar(positive=['queen', 'man'], negative=['woman'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now lets try what such a model can do on one of those IQ quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors.wv.doesnt_match([\"apple\", \"banana\", \"orange\", \"bread\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "___\n",
    "\n",
    "![](autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence to sequence models\n",
    "___\n",
    "![](encdec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](seq2seq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory Networks\n",
    "___\n",
    "\n",
    "* https://arxiv.org/abs/1410.3916\n",
    "* https://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What can they do?\n",
    "\n",
    "* Joe went to the *kitchen*. Fred went to the *kitchen*. Joe picked up the *milk*.\n",
    "* Joe travelled to the *office*. Joe left the *milk*. Joe went to the *bathroom*.\n",
    "* **Where is the milk now?** A: *office*\n",
    "* **Where is Joe?** A: *bathroom*\n",
    "* **Where was Joe before the office?** A: *kitchen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Wtf how?\n",
    "![](study-of-end-to-end-memory-networks-9-638.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](thank-you-for-your-attention-now-its-time-for-questions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sources of some of the images:\n",
    "\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd\n",
    "* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "* https://www.celebros.com/blog/five-benefits-to-choosing-natural-language-processing-for-on-site-search.html\n",
    "* https://www.tensorflow.org/tutorials/seq2seq\n",
    "* https://image.slidesharecdn.com/presentationrev1-170221023043/95/study-of-end-to-end-memory-networks-9-638.jpg?cb=1487644455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful references:\n",
    "\n",
    "* https://github.com/fmi/machine-learning-lectures\n",
    "* https://www.kaggle.com/\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://en.wikipedia.org/wiki/Feature_hashing\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "* https://www.tensorflow.org/tutorials/seq2seq\n",
    "* https://github.com/facebook/MemNN\n",
    "* https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/\n",
    "* https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>All links above are interesting and useful if you want to deep dive in the world of machine learning.</center>\n",
    "\n",
    " ### <center>The end, thanks!</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
