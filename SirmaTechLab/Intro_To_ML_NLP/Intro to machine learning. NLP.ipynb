{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Introduction to machine learning.</center></h1>\n",
    "   <h1><center> Natural language processing.</center></h1>\n",
    "   \n",
    "*Radoslav Petkov*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### About me\n",
    "\n",
    "* Contact: https://www.linkedin.com/in/radoslav-petkov-8a4a53144/\n",
    "* Sofia University, Computer Science, Bsc (2nd year)\n",
    "* Has been working for Sirma since end of 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "* Supervised vs Unsupervised machine learning\n",
    "* Representation of words and sentences\n",
    "* Autoencoders\n",
    "* Sequence to sequence models\n",
    "* Memory Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised vs Unsupervised\n",
    "\n",
    "<img src=\"un_supervised.png\" alt = \"un_supervised\" style = \"width:1182px; height=702px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# <center>Supervised</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# include boston houses dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# include sth interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What the computer sees?<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### What is one hot encoding of words?\n",
    "\n",
    "Simply said, it is representation where each word is vector with size equal to the vocabulary size and there is 1 at the index equal to the word index.\n",
    "\n",
    "Imagine you have the following corpus of words: *Machine*, *learning*, *rocks*.\n",
    "\n",
    "***One representation would be :***\n",
    "\n",
    "Machine -> [1 0 0],  learning -> [0 1 0], rocks -> [0 0 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ### What about Bag of Words?\n",
    "\n",
    "Each sentence is vector with size equal to the vocablary size storing the occurances of each word in the sentence.\n",
    "There are several additional modifications such as:\n",
    "* **TF-IDF**\n",
    "\n",
    "    We assign weights of each word instead of occurances.The weights tend to filter out common terms. If the weight is 0 then the word is present in every sentence.\n",
    "    TF is the count of the word in the current sentence.\n",
    "    IDF is the count of the word in all sentences we are working with.\n",
    "* **Hashing**\n",
    "\n",
    "    Instead of using dictionary to vectorize, a special hash function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "Lets have the following corpus: *machine*, *learning*, *rocks*, *this*, *robot*\n",
    "\n",
    "And the following sentences:\n",
    "\n",
    "* *machine learning rocks*\n",
    "* *this machine learning rocks*\n",
    "* *this robot rocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"machine learning rocks\", \"this machine learning rocks\", \"this robot rocks\"]\n",
    "corpus_transformed = CountVectorizer().fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning rocks [1 1 0 1 0]\n",
      "this machine learning rocks [1 1 0 1 1]\n",
      "this robot rocks [0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "for (sentence, transf) in zip(corpus, corpus_transformed.toarray()):\n",
    "    print(sentence, transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*  ### Hmm, now the fancy word vectors\n",
    "\n",
    "Vectors from lattent space formed by the corpus used for training unsupervised model.\n",
    "They can catch the 'semantic meaning of a word'.\n",
    "\n",
    "<img src=\"./word_embedding2.jpg\" alt = \"word_embeddings\" style = \"float:right;width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./word_embedding.png\" alt = \"word_embeddings\" style=\"width:800px; height:auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/media/radoslav/6906F83679A14133/Download/glove/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('walked', 0.751861572265625)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.most_similar(positive=['walking', 'swam'], negative=['swimming'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bulgaria', 0.7505655288696289)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.most_similar(positive=['France', 'Sofia'], negative=['Paris'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('king', 0.6958590149879456)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.most_similar(positive=['queen', 'man'], negative=['woman'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now lets try what a model can do on one of those IQ quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bread'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv.doesnt_match([\"apple\", \"banana\", \"orange\", \"bread\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Autoencoders</center>\n",
    "\n",
    "<img src=\"autoencoder.png\" alt = \"un_supervised\" style = \"width:1182px; height=702px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Sequence to sequence models</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"thank-you-for-your-attention-now-its-time-for-questions.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sources of some of the images:\n",
    "\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd\n",
    "* https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful references:\n",
    "\n",
    "* https://github.com/fmi/machine-learning-lectures\n",
    "* https://www.kaggle.com/\n",
    "* https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/machine_learning.html\n",
    "* https://en.wikipedia.org/wiki/Feature_hashing\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>All links above are interesting and useful if you want to deep dive in the world of machine learning.</center>\n",
    "\n",
    " ### <center>The end, thanks!</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
